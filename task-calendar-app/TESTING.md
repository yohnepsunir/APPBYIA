# Task Calendar App - GuÃ­a de Testing

## ðŸ“‹ Tabla de Contenidos

- [IntroducciÃ³n](#introducciÃ³n)
- [Estructura de Tests](#estructura-de-tests)
- [InstalaciÃ³n](#instalaciÃ³n)
- [Ejecutar Tests](#ejecutar-tests)
- [Tipos de Tests](#tipos-de-tests)
- [Coverage](#coverage)
- [CI/CD](#cicd)
- [Mejores PrÃ¡cticas](#mejores-prÃ¡cticas)

## ðŸŽ¯ IntroducciÃ³n

Este proyecto cuenta con un conjunto completo de pruebas unitarias e integraciÃ³n que garantizan la calidad y funcionamiento correcto de la aplicaciÃ³n de calendario de tareas.

### TecnologÃ­as Utilizadas

- **pytest**: Framework de testing principal
- **pytest-cov**: Plugin para cobertura de cÃ³digo
- **pytest-flask**: Utilidades para testing de Flask
- **pytest-mock**: Mocking simplificado
- **coverage**: AnÃ¡lisis de cobertura de cÃ³digo

## ðŸ“ Estructura de Tests

```
backend/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py              # Fixtures compartidas
â”‚   â”œâ”€â”€ unit/                    # Tests unitarios
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_database.py     # Tests de base de datos
â”‚   â”‚   â”œâ”€â”€ test_models.py       # Tests de modelos
â”‚   â”‚   â””â”€â”€ test_routes.py       # Tests de rutas/endpoints
â”‚   â””â”€â”€ integration/             # Tests de integraciÃ³n
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_app_integration.py     # Tests de la app completa
â”‚       â””â”€â”€ test_task_workflow.py       # Tests de flujos de trabajo
â”œâ”€â”€ pytest.ini                   # ConfiguraciÃ³n de pytest
â”œâ”€â”€ .coveragerc                  # ConfiguraciÃ³n de coverage
â”œâ”€â”€ run_tests.sh                 # Script para Linux/Mac
â””â”€â”€ run_tests.bat                # Script para Windows
```

## ðŸš€ InstalaciÃ³n

### Instalar Dependencias

```bash
cd backend
pip install -r requirements.txt
```

Las dependencias de testing incluyen:
- pytest==7.4.3
- pytest-cov==4.1.0
- pytest-flask==1.3.0
- pytest-mock==3.12.0
- coverage==7.3.2

## â–¶ï¸ Ejecutar Tests

### OpciÃ³n 1: Scripts de Conveniencia

#### Linux/Mac:
```bash
cd backend
chmod +x run_tests.sh
./run_tests.sh                 # Todos los tests con coverage
./run_tests.sh --unit          # Solo tests unitarios
./run_tests.sh --integration   # Solo tests de integraciÃ³n
./run_tests.sh --fast          # Tests sin coverage (mÃ¡s rÃ¡pido)
./run_tests.sh --verbose       # Modo verbose
```

#### Windows:
```cmd
cd backend
run_tests.bat                  # Todos los tests con coverage
run_tests.bat --unit           # Solo tests unitarios
run_tests.bat --integration    # Solo tests de integraciÃ³n
run_tests.bat --fast           # Tests sin coverage
```

### OpciÃ³n 2: Comandos Directos con pytest

```bash
cd backend

# Ejecutar todos los tests
pytest

# Tests unitarios con coverage
pytest tests/unit/ -v --cov=. --cov-report=term-missing

# Tests de integraciÃ³n
pytest tests/integration/ -v

# Test especÃ­fico
pytest tests/unit/test_models.py::TestTaskModel::test_task_creation -v

# Tests con markers
pytest -m unit              # Solo tests marcados como 'unit'
pytest -m integration       # Solo tests marcados como 'integration'
pytest -m "not slow"        # Excluir tests lentos
```

### OpciÃ³n 3: Con Docker

```bash
# Construir imagen
docker-compose build

# Ejecutar tests en container
docker-compose run backend python -m pytest tests/ -v

# Ejecutar con coverage
docker-compose run backend python -m pytest tests/ -v --cov=. --cov-report=term-missing
```

## ðŸ§ª Tipos de Tests

### Tests Unitarios

Prueban componentes individuales de forma aislada:

- **test_database.py**: Pruebas de conexiÃ³n y estructura de base de datos
- **test_models.py**: Pruebas de modelos (CRUD de tareas)
- **test_routes.py**: Pruebas de endpoints de la API

**Ejemplo de ejecuciÃ³n:**
```bash
pytest tests/unit/ -v
```

### Tests de IntegraciÃ³n

Prueban el flujo completo de la aplicaciÃ³n:

- **test_app_integration.py**: IntegraciÃ³n de toda la aplicaciÃ³n
- **test_task_workflow.py**: Flujos de trabajo completos (crear â†’ actualizar â†’ eliminar)

**Ejemplo de ejecuciÃ³n:**
```bash
pytest tests/integration/ -v
```

## ðŸ“Š Coverage (Cobertura de CÃ³digo)

### Generar Reporte de Cobertura

```bash
cd backend

# Generar coverage en terminal
pytest --cov=. --cov-report=term-missing

# Generar reporte HTML
pytest --cov=. --cov-report=html

# Generar reporte XML (para CI/CD)
pytest --cov=. --cov-report=xml

# Ver reporte HTML en navegador
# El archivo se genera en: htmlcov/index.html
```

### Interpretar Reportes de Coverage

- **Stmts**: NÃºmero total de lÃ­neas de cÃ³digo
- **Miss**: LÃ­neas no cubiertas por tests
- **Cover**: Porcentaje de cobertura
- **Missing**: NÃºmeros de lÃ­nea especÃ­ficos no cubiertos

**Objetivo de cobertura**: >= 80%

### Archivos de ConfiguraciÃ³n

**`.coveragerc`**: Configura quÃ© archivos incluir/excluir
- Excluye directorios de tests, venv, __pycache__
- Configura formato de reportes
- Define lÃ­neas a ignorar (pragma: no cover)

## ðŸ”„ CI/CD

### GitHub Actions

El proyecto incluye un workflow de CI/CD en `.github/workflows/tests.yml`:

**CaracterÃ­sticas:**
- âœ… Ejecuta tests en mÃºltiples versiones de Python (3.9, 3.10, 3.11)
- âœ… Tests unitarios e integraciÃ³n
- âœ… AnÃ¡lisis de cobertura
- âœ… Linting (flake8, black, isort)
- âœ… Build y test de Docker
- âœ… Upload de reportes a Codecov

**Se ejecuta en:**
- Push a `main` o `develop`
- Pull requests a `main` o `develop`

### Badges de Estado

Agrega estos badges a tu README.md:

```markdown
![Tests](https://github.com/yohnepsunir/APPBYIA/workflows/Tests%20CI%2FCD/badge.svg)
[![codecov](https://codecov.io/gh/yohnepsunir/APPBYIA/branch/main/graph/badge.svg)](https://codecov.io/gh/yohnepsunir/APPBYIA)
```

## ðŸ“ Mejores PrÃ¡cticas

### 1. Escribir Tests Efectivos

```python
# âœ… BIEN: Test especÃ­fico y descriptivo
def test_task_creation_with_valid_data(self, db_connection, sample_task_data):
    """Test que verifica la creaciÃ³n exitosa de una tarea con datos vÃ¡lidos"""
    task_id = Task.create(**sample_task_data)
    assert task_id > 0
    
    task = Task.get_by_id(task_id)
    assert task['title'] == sample_task_data['title']

# âŒ MAL: Test vago y sin verificaciones suficientes
def test_task(self):
    task_id = Task.create('Test', '', '', 1, '')
    assert task_id
```

### 2. Usar Fixtures Apropiadamente

```python
# Definir en conftest.py
@pytest.fixture(scope='function')
def sample_task_data():
    return {
        'title': 'Test Task',
        'description': 'Test description',
        'category': 'work',
        'priority': 3,
        'due_date': '2025-12-31'
    }

# Usar en tests
def test_create_task(self, sample_task_data):
    task_id = Task.create(**sample_task_data)
    assert task_id > 0
```

### 3. OrganizaciÃ³n de Tests

- **Un archivo de test por mÃ³dulo** que estÃ¡s probando
- **Una clase de test por clase** del cÃ³digo
- **Nombres descriptivos** que explican quÃ© se estÃ¡ probando
- **Arrange-Act-Assert** pattern

```python
def test_task_update(self):
    # Arrange: Preparar datos
    task_id = Task.create('Original', 'Desc', 'cat', 3, '2025-12-31')
    
    # Act: Ejecutar acciÃ³n
    Task.update(task_id, 'Updated', 'Desc', 'cat', 3, '2025-12-31', 'completed')
    
    # Assert: Verificar resultado
    task = Task.get_by_id(task_id)
    assert task['title'] == 'Updated'
    assert task['status'] == 'completed'
```

### 4. Tests Independientes

Cada test debe poder ejecutarse de forma independiente:

```python
# âœ… BIEN: Crea sus propios datos
def test_get_task(self, db_connection):
    task_id = Task.create('Test', '', '', 3, '')
    task = Task.get_by_id(task_id)
    assert task is not None

# âŒ MAL: Depende de datos de otros tests
def test_get_task_bad(self):
    task = Task.get_by_id(1)  # Asume que existe
    assert task is not None
```

### 5. Manejo de Base de Datos en Tests

```python
# Usar fixture que crea DB limpia para cada test
@pytest.fixture(scope='function')
def db_connection():
    db_fd, db_path = tempfile.mkstemp()
    database.DATABASE = db_path
    init_db()
    
    yield db_path
    
    # Cleanup
    os.close(db_fd)
    os.unlink(db_path)
```

## ðŸ› Debugging de Tests

### Ver output detallado
```bash
pytest -v -s  # -s muestra prints
```

### Ejecutar tests hasta el primer fallo
```bash
pytest -x
```

### Ejecutar solo tests que fallaron la Ãºltima vez
```bash
pytest --lf
```

### Debugger interactivo
```bash
pytest --pdb  # Abre debugger en fallos
```

### Ver traceback completo
```bash
pytest --tb=long
```

## ðŸ“ˆ MÃ©tricas y Reportes

### EstadÃ­sticas de Tests
```bash
pytest --durations=10  # Muestra los 10 tests mÃ¡s lentos
```

### Generar Reporte JUnit (para CI)
```bash
pytest --junitxml=junit.xml
```

### AnÃ¡lisis de Cobertura Detallado
```bash
coverage report --show-missing
coverage html
```

## ðŸ”§ Troubleshooting

### Problema: Tests fallan con "ModuleNotFoundError"
**SoluciÃ³n**: AsegÃºrate de ejecutar pytest desde el directorio `backend/`

### Problema: Base de datos bloqueada
**SoluciÃ³n**: Verifica que los fixtures cierren las conexiones correctamente

### Problema: Tests pasan individualmente pero fallan en conjunto
**SoluciÃ³n**: Revisa que los tests sean independientes y no compartan estado

### Problema: Coverage bajo
**SoluciÃ³n**: 
- Ejecuta `coverage report --show-missing` para ver lÃ­neas no cubiertas
- Agrega tests para casos edge y error handling

## ðŸ“š Recursos Adicionales

- [DocumentaciÃ³n de pytest](https://docs.pytest.org/)
- [pytest-flask](https://pytest-flask.readthedocs.io/)
- [Coverage.py](https://coverage.readthedocs.io/)
- [Testing Flask Applications](https://flask.palletsprojects.com/en/2.3.x/testing/)

## ðŸ¤ Contribuir

Al agregar nuevas funcionalidades:

1. âœ… Escribe tests ANTES de implementar (TDD)
2. âœ… Asegura coverage >= 80%
3. âœ… Ejecuta todos los tests antes de commit
4. âœ… Actualiza esta documentaciÃ³n si es necesario

---

**Ãšltima actualizaciÃ³n**: Diciembre 2025
